# N-Gram-Autocomplete

![alt text](https://github.com/BheZelmat/N-Gram-Autocomplete/blob/main/img.png?raw=true)

## Overview
This notebook presents an in-depth exploration of building an N-gram auto-complete system, leveraging language models to predict the next word in a sequence. It incorporates a detailed walkthrough from data preprocessing to model evaluation and implementation.

## Key Features
Detailed data preprocessing to transform Twitter text data for N-gram model training.
Construction of unigram to N-gram models to capture context.
Implementation of smoothing techniques to enhance model performance.
Perplexity calculation to assess model accuracy.
Development of an auto-complete function, offering real-time predictive text suggestions.

## Dependencies
Python
NumPy
Pandas
NLTK
Dataset
Utilizes a corpus of Twitter data, illustrating the model's capability to handle real-world, informal language.

## Usage
Follow the detailed steps within the notebook to replicate the auto-complete system or adapt it to your dataset.


## Author 
B Houssem E Zelmat 
